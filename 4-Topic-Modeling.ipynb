{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular text analysis technique is called topic modeling. The ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics.\n",
    "\n",
    "In this notebook, we will be covering the steps on how to do **Latent Dirichlet Allocation (LDA)**, which is one of many topic modeling techniques. It was specifically designed for text data.\n",
    "\n",
    "To use a topic modeling technique, you need to provide (1) a document-term matrix and (2) the number of topics you would like the algorithm to pick up.\n",
    "\n",
    "Once the topic modeling technique is applied, your job as a human is to interpret the results and see if the mix of words in each topic make sense. If they don't make sense, you can try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #1 (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaahhh</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aah</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abc</th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abnormal</th>\n",
       "      <th>...</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zorro</th>\n",
       "      <th>zuck</th>\n",
       "      <th>zuckerbergs</th>\n",
       "      <th>álvarez</th>\n",
       "      <th>ándale</th>\n",
       "      <th>ñañaras</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>david</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>george</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jon</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kate</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kevin</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leanne</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lewis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pete</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roseanne</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sammy</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shane</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stavros</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 8112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          aaaaah  aaaahhh  aaah  aah  abandoned  abc  abilities  ability  \\\n",
       "david          0        0     0    0          0    0          0        0   \n",
       "gabriel        0        0     1    0          0    1          0        0   \n",
       "george         0        0     0    0          0    0          0        1   \n",
       "jon            0        0     0    0          0    0          0        0   \n",
       "kate           0        0     0    1          0    0          0        0   \n",
       "kevin          0        0     0    0          0    0          0        0   \n",
       "leanne         0        0     0    0          1    0          0        0   \n",
       "lewis          0        0     0    0          0    0          0        0   \n",
       "louis          0        1     0    4          0    0          0        0   \n",
       "matt           0        0     0    0          0    0          0        0   \n",
       "pete           0        0     0    1          0    0          0        1   \n",
       "roseanne       0        0     0    0          0    4          1        0   \n",
       "sammy          0        0     0    0          0    0          0        0   \n",
       "shane          0        0     0    0          0    0          0        0   \n",
       "stavros        1        0     0    0          0    0          0        0   \n",
       "\n",
       "          able  abnormal  ...  zone  zones  zoo  zoom  zorro  zuck  \\\n",
       "david        0         0  ...     0      0    0     0      3     0   \n",
       "gabriel      0         0  ...     0      1    0     4      0     0   \n",
       "george       3         0  ...     0      0    0     0      0     1   \n",
       "jon          2         0  ...     1      0    0     0      0     0   \n",
       "kate         2         0  ...     0      0    0     0      0     0   \n",
       "kevin        0         0  ...     0      0    0     0      0     0   \n",
       "leanne       0         0  ...     0      0    0     0      0     0   \n",
       "lewis        1         4  ...     0      0    0     0      0     0   \n",
       "louis        2         0  ...     0      0    0     0      0     0   \n",
       "matt         3         0  ...     1      0    0     0      0     0   \n",
       "pete         1         0  ...     1      0    0     1      0     0   \n",
       "roseanne     8         0  ...     0      0    0     0      0     0   \n",
       "sammy        0         0  ...     0      0    0     0      0     0   \n",
       "shane        0         0  ...     0      0    1     0      0     0   \n",
       "stavros      2         0  ...     0      0    0     0      0     0   \n",
       "\n",
       "          zuckerbergs  álvarez  ándale  ñañaras  \n",
       "david               0        0       0        0  \n",
       "gabriel             0        3       1        1  \n",
       "george              1        0       0        0  \n",
       "jon                 0        0       0        0  \n",
       "kate                0        0       0        0  \n",
       "kevin               0        0       0        0  \n",
       "leanne              0        0       0        0  \n",
       "lewis               0        0       0        0  \n",
       "louis               0        0       0        0  \n",
       "matt                0        0       0        0  \n",
       "pete                0        0       0        0  \n",
       "roseanne            0        0       0        0  \n",
       "sammy               0        0       0        0  \n",
       "shane               0        0       0        0  \n",
       "stavros             0        0       0        0  \n",
       "\n",
       "[15 rows x 8112 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>david</th>\n",
       "      <th>gabriel</th>\n",
       "      <th>george</th>\n",
       "      <th>jon</th>\n",
       "      <th>kate</th>\n",
       "      <th>kevin</th>\n",
       "      <th>leanne</th>\n",
       "      <th>lewis</th>\n",
       "      <th>louis</th>\n",
       "      <th>matt</th>\n",
       "      <th>pete</th>\n",
       "      <th>roseanne</th>\n",
       "      <th>sammy</th>\n",
       "      <th>shane</th>\n",
       "      <th>stavros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaaaah</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaahhh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaah</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aah</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandoned</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           david  gabriel  george  jon  kate  kevin  leanne  lewis  louis  \\\n",
       "aaaaah         0        0       0    0     0      0       0      0      0   \n",
       "aaaahhh        0        0       0    0     0      0       0      0      1   \n",
       "aaah           0        1       0    0     0      0       0      0      0   \n",
       "aah            0        0       0    0     1      0       0      0      4   \n",
       "abandoned      0        0       0    0     0      0       1      0      0   \n",
       "\n",
       "           matt  pete  roseanne  sammy  shane  stavros  \n",
       "aaaaah        0     0         0      0      0        1  \n",
       "aaaahhh       0     0         0      0      0        0  \n",
       "aaah          0     0         0      0      0        0  \n",
       "aah           0     1         0      0      0        0  \n",
       "abandoned     0     0         0      0      0        0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes. Let's start the number of topics at 2, see if the results make sense, and increase the number from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"fucking\" + 0.007*\"okay\" + 0.005*\"life\" + 0.005*\"fuck\" + 0.005*\"theres\" + 0.004*\"didnt\" + 0.004*\"did\" + 0.004*\"mean\" + 0.004*\"cause\" + 0.004*\"come\"'),\n",
       " (1,\n",
       "  '0.008*\"audience\" + 0.006*\"laughing\" + 0.005*\"did\" + 0.004*\"thing\" + 0.004*\"cause\" + 0.004*\"shes\" + 0.004*\"okay\" + 0.004*\"make\" + 0.004*\"didnt\" + 0.004*\"tell\"')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=50)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"okay\" + 0.005*\"did\" + 0.005*\"thank\" + 0.004*\"tell\" + 0.004*\"ive\" + 0.004*\"theres\" + 0.004*\"make\" + 0.004*\"thing\" + 0.004*\"look\" + 0.004*\"cause\"'),\n",
       " (1,\n",
       "  '0.008*\"audience\" + 0.005*\"laughing\" + 0.005*\"okay\" + 0.005*\"did\" + 0.005*\"fucking\" + 0.004*\"didnt\" + 0.004*\"life\" + 0.004*\"cause\" + 0.004*\"shes\" + 0.004*\"goes\"')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=50)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights you see in the LDA topic output (ldana.print_topics()) change slightly every time you run the code because of inherent randomness in the LDA algorithm. Here's why:\n",
    "\n",
    "Random initialization: LDA often uses random initialization for internal variables like topic assignments for each word. This starting point can influence the final converged model slightly.\n",
    "Optimization process: LDA uses an iterative optimization process to find the best topic assignments for words. While the algorithm converges towards a good solution, slight variations in the path it takes to reach that point can lead to minor weight differences.\n",
    "These factors introduce a small degree of randomness in the weights, causing them to fluctuate slightly across different runs. However, the overall thematic trends identified by the model should remain consistent.\n",
    "\n",
    "Here are some ways to potentially reduce the weight variation:\n",
    "\n",
    "Set a random seed: LDA libraries often allow setting a random seed for the initialization process. This ensures the starting point is the same for each run, leading to more consistent results (though some randomness might still be present in the optimization).\n",
    "Increase the number of passes: Increasing the number of iterations (passes) in the LDA model can allow it to converge more thoroughly, potentially reducing weight variations. However, this also increases computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.017*\"audience\" + 0.012*\"laughing\" + 0.006*\"irish\" + 0.004*\"bit\" + 0.004*\"world\" + 0.004*\"applauding\" + 0.004*\"america\" + 0.004*\"years\" + 0.004*\"didnt\" + 0.004*\"did\"'),\n",
       " (1,\n",
       "  '0.009*\"okay\" + 0.008*\"fucking\" + 0.005*\"fuck\" + 0.005*\"cause\" + 0.005*\"did\" + 0.005*\"tell\" + 0.005*\"shes\" + 0.005*\"didnt\" + 0.004*\"life\" + 0.004*\"goes\"'),\n",
       " (2,\n",
       "  '0.006*\"did\" + 0.005*\"theres\" + 0.004*\"life\" + 0.004*\"shes\" + 0.004*\"okay\" + 0.004*\"ive\" + 0.004*\"mean\" + 0.004*\"tell\" + 0.004*\"doing\" + 0.004*\"love\"')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=100)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"did\" + 0.006*\"guys\" + 0.005*\"fcking\" + 0.005*\"shes\" + 0.005*\"dude\" + 0.005*\"dad\" + 0.004*\"guy\" + 0.004*\"theres\" + 0.004*\"life\" + 0.004*\"doing\"'),\n",
       " (1,\n",
       "  '0.016*\"audience\" + 0.010*\"laughing\" + 0.008*\"fucking\" + 0.006*\"didnt\" + 0.005*\"did\" + 0.004*\"shit\" + 0.004*\"irish\" + 0.004*\"fuck\" + 0.004*\"thing\" + 0.004*\"love\"'),\n",
       " (2,\n",
       "  '0.008*\"okay\" + 0.005*\"theres\" + 0.005*\"cause\" + 0.005*\"life\" + 0.004*\"goes\" + 0.004*\"make\" + 0.004*\"did\" + 0.004*\"shes\" + 0.004*\"fucking\" + 0.004*\"tell\"'),\n",
       " (3,\n",
       "  '0.006*\"okay\" + 0.005*\"tell\" + 0.005*\"look\" + 0.005*\"did\" + 0.004*\"thank\" + 0.004*\"thing\" + 0.004*\"cause\" + 0.004*\"make\" + 0.004*\"guys\" + 0.004*\"way\"')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=100)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics aren't looking too great. We've tried modifying our parameters. Let's try modifying our terms list as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #2 (Nouns Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One popular trick is to look only at terms that are from one part of speech (only nouns, only adjectives, etc.). Check out the UPenn tag set: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>david</th>\n",
       "      <td>in this  comedy special david nihill humorousl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>can you please state your name martin moreno ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>george</th>\n",
       "      <td>george carlin im glad im dead  is a controvers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jon</th>\n",
       "      <td>in an interview conducted by jon stewart georg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kate</th>\n",
       "      <td>whoa okay yeah good okay dont embarrass yourse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kevin</th>\n",
       "      <td>kevin james irregardless  in kevin james irreg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leanne</th>\n",
       "      <td>leanne morgan im every woman  in im every woma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lewis</th>\n",
       "      <td>lewis black tragically i need you  is a standu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>louis ck at the dolby is louis cks third selfr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matt</th>\n",
       "      <td>in his second hourlong comedy special matthew ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pete</th>\n",
       "      <td>pete davidson turbo fonzarelli released date j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roseanne</th>\n",
       "      <td>roseanne barr cancel this is a standup comedy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sammy</th>\n",
       "      <td>in this humorous and politically charged stand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shane</th>\n",
       "      <td>shane gillis debut standup comedy special reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stavros</th>\n",
       "      <td>in his standup comedy special live at the lodg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 transcript\n",
       "david     in this  comedy special david nihill humorousl...\n",
       "gabriel    can you please state your name martin moreno ...\n",
       "george    george carlin im glad im dead  is a controvers...\n",
       "jon       in an interview conducted by jon stewart georg...\n",
       "kate      whoa okay yeah good okay dont embarrass yourse...\n",
       "kevin     kevin james irregardless  in kevin james irreg...\n",
       "leanne    leanne morgan im every woman  in im every woma...\n",
       "lewis     lewis black tragically i need you  is a standu...\n",
       "louis     louis ck at the dolby is louis cks third selfr...\n",
       "matt      in his second hourlong comedy special matthew ...\n",
       "pete      pete davidson turbo fonzarelli released date j...\n",
       "roseanne  roseanne barr cancel this is a standup comedy ...\n",
       "sammy     in this humorous and politically charged stand...\n",
       "shane     shane gillis debut standup comedy special reco...\n",
       "stavros   in his standup comedy special live at the lodg..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:\n",
    "\n",
    "Divides a block of text into a list of sentences.\n",
    "Handles cases where periods (.) might not indicate sentence boundaries (e.g., abbreviations, Mr.)\n",
    "Accounts for sentences that don't begin with capital letters (e.g., dialogue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The averaged_perceptron_tagger is a part of the Natural Language Processing Toolkit (NLTK) library in Python. It's a powerful tool for Part-of-Speech (POS) tagging, which involves assigning grammatical labels (nouns, verbs, adjectives, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>david</th>\n",
       "      <td>comedy david nihill complexities identity norm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>state name martin moreno martinnnnn gabriel ig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>george</th>\n",
       "      <td>george carlin im im dead project debate concer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jon</th>\n",
       "      <td>interview stewart george carlin talks aspects ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kate</th>\n",
       "      <td>whoa okay okay dont embarrass expectations i w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kevin</th>\n",
       "      <td>kevin james kevin james standup comedy kevin j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leanne</th>\n",
       "      <td>leanne morgan woman im woman delivers performa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lewis</th>\n",
       "      <td>comedy show signature mix sarcasm outrage impa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>louis ck dolby cks standup comedy pandemic gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matt</th>\n",
       "      <td>comedy matthew stephen rife rife performance n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pete</th>\n",
       "      <td>davidson turbo fonzarelli date netflixruntime ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roseanne</th>\n",
       "      <td>roseanne barr cancel comedy roseanne barrs ret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sammy</th>\n",
       "      <td>standup segment sammy obeid shares connections...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shane</th>\n",
       "      <td>shane gillis comedy creek cave austin tx blend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stavros</th>\n",
       "      <td>standup comedy live lodge room stavros halkias...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 transcript\n",
       "david     comedy david nihill complexities identity norm...\n",
       "gabriel   state name martin moreno martinnnnn gabriel ig...\n",
       "george    george carlin im im dead project debate concer...\n",
       "jon       interview stewart george carlin talks aspects ...\n",
       "kate      whoa okay okay dont embarrass expectations i w...\n",
       "kevin     kevin james kevin james standup comedy kevin j...\n",
       "leanne    leanne morgan woman im woman delivers performa...\n",
       "lewis     comedy show signature mix sarcasm outrage impa...\n",
       "louis     louis ck dolby cks standup comedy pandemic gra...\n",
       "matt      comedy matthew stephen rife rife performance n...\n",
       "pete      davidson turbo fonzarelli date netflixruntime ...\n",
       "roseanne  roseanne barr cancel comedy roseanne barrs ret...\n",
       "sammy     standup segment sammy obeid shares connections...\n",
       "shane     shane gillis comedy creek cave austin tx blend...\n",
       "stavros   standup comedy live lodge room stavros halkias..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaahhh</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>abolitionist</th>\n",
       "      <th>abomination</th>\n",
       "      <th>abortion</th>\n",
       "      <th>...</th>\n",
       "      <th>zionists</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zorro</th>\n",
       "      <th>zuck</th>\n",
       "      <th>zuckerbergs</th>\n",
       "      <th>álvarez</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>david</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>george</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jon</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kate</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kevin</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leanne</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lewis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pete</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roseanne</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sammy</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shane</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stavros</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 4979 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          aaaaah  aaaahhh  aaah  aah  abc  abilities  ability  abolitionist  \\\n",
       "david          0        0     0    0    0          0        0             1   \n",
       "gabriel        0        0     1    0    1          0        0             0   \n",
       "george         0        0     0    0    0          0        1             0   \n",
       "jon            0        0     0    0    0          0        0             0   \n",
       "kate           0        0     0    1    0          0        0             0   \n",
       "kevin          0        0     0    0    0          0        0             0   \n",
       "leanne         0        0     0    0    0          0        0             0   \n",
       "lewis          0        0     0    0    0          0        0             0   \n",
       "louis          0        1     0    2    0          0        0             0   \n",
       "matt           0        0     0    0    0          0        0             0   \n",
       "pete           0        0     0    1    0          0        1             0   \n",
       "roseanne       0        0     0    0    3          1        0             0   \n",
       "sammy          0        0     0    0    0          0        0             0   \n",
       "shane          0        0     0    0    0          0        0             0   \n",
       "stavros        1        0     0    0    0          0        0             0   \n",
       "\n",
       "          abomination  abortion  ...  zionists  zip  zone  zones  zoo  zoom  \\\n",
       "david               0         0  ...         0    0     0      0    0     0   \n",
       "gabriel             0         0  ...         0    0     0      1    0     3   \n",
       "george              1         0  ...         0    0     0      0    0     0   \n",
       "jon                 0         0  ...         0    0     1      0    0     0   \n",
       "kate                0         0  ...         0    0     0      0    0     0   \n",
       "kevin               0         0  ...         0    0     0      0    0     0   \n",
       "leanne              0         0  ...         0    1     0      0    0     0   \n",
       "lewis               0         0  ...         0    0     0      0    0     0   \n",
       "louis               0         7  ...         0    0     0      0    0     0   \n",
       "matt                0         0  ...         0    0     1      0    0     0   \n",
       "pete                0         0  ...         0    0     1      0    0     1   \n",
       "roseanne            0         1  ...         0    0     0      0    0     0   \n",
       "sammy               0         0  ...         1    0     0      0    0     0   \n",
       "shane               0         0  ...         0    0     0      0    1     0   \n",
       "stavros             0         0  ...         0    0     0      0    0     0   \n",
       "\n",
       "          zorro  zuck  zuckerbergs  álvarez  \n",
       "david         3     0            0        0  \n",
       "gabriel       0     0            0        3  \n",
       "george        0     1            1        0  \n",
       "jon           0     0            0        0  \n",
       "kate          0     0            0        0  \n",
       "kevin         0     0            0        0  \n",
       "leanne        0     0            0        0  \n",
       "lewis         0     0            0        0  \n",
       "louis         0     0            0        0  \n",
       "matt          0     0            0        0  \n",
       "pete          0     0            0        0  \n",
       "roseanne      0     0            0        0  \n",
       "sammy         0     0            0        0  \n",
       "shane         0     0            0        0  \n",
       "stavros       0     0            0        0  \n",
       "\n",
       "[15 rows x 4979 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = list(text.ENGLISH_STOP_WORDS.union(add_stop_words))  # Convert frozenset to list\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names_out())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"audience\" + 0.009*\"hes\" + 0.008*\"thing\" + 0.008*\"life\" + 0.007*\"way\" + 0.007*\"day\" + 0.007*\"shes\" + 0.006*\"gon\" + 0.006*\"man\" + 0.006*\"years\"'),\n",
       " (1,\n",
       "  '0.007*\"life\" + 0.007*\"hes\" + 0.006*\"way\" + 0.006*\"day\" + 0.006*\"lot\" + 0.006*\"cause\" + 0.006*\"thing\" + 0.005*\"shes\" + 0.005*\"gon\" + 0.005*\"theyre\"')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=100)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.020*\"audience\" + 0.008*\"hes\" + 0.008*\"thing\" + 0.007*\"way\" + 0.007*\"cause\" + 0.007*\"life\" + 0.006*\"okay\" + 0.006*\"years\" + 0.006*\"gon\" + 0.005*\"day\"'),\n",
       " (1,\n",
       "  '0.009*\"life\" + 0.008*\"hes\" + 0.007*\"day\" + 0.007*\"shes\" + 0.007*\"thing\" + 0.007*\"lot\" + 0.006*\"guy\" + 0.006*\"way\" + 0.006*\"gon\" + 0.006*\"theyre\"')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=100)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"hes\" + 0.009*\"cause\" + 0.009*\"okay\" + 0.008*\"way\" + 0.008*\"thing\" + 0.007*\"shes\" + 0.007*\"life\" + 0.007*\"day\" + 0.005*\"gon\" + 0.005*\"home\"'),\n",
       " (1,\n",
       "  '0.010*\"life\" + 0.008*\"hes\" + 0.008*\"man\" + 0.006*\"years\" + 0.006*\"day\" + 0.006*\"jesus\" + 0.006*\"shit\" + 0.006*\"mom\" + 0.006*\"lot\" + 0.006*\"guy\"'),\n",
       " (2,\n",
       "  '0.021*\"audience\" + 0.008*\"gon\" + 0.008*\"thing\" + 0.007*\"way\" + 0.007*\"life\" + 0.007*\"day\" + 0.007*\"hes\" + 0.006*\"lot\" + 0.006*\"shes\" + 0.006*\"kind\"')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=100)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.010*\"life\" + 0.009*\"hes\" + 0.009*\"okay\" + 0.007*\"mom\" + 0.007*\"man\" + 0.007*\"day\" + 0.007*\"shes\" + 0.006*\"way\" + 0.006*\"guy\" + 0.006*\"lot\"'),\n",
       " (1,\n",
       "  '0.029*\"audience\" + 0.008*\"gon\" + 0.008*\"way\" + 0.008*\"life\" + 0.008*\"cause\" + 0.007*\"bit\" + 0.007*\"lot\" + 0.007*\"day\" + 0.007*\"thing\" + 0.007*\"shes\"'),\n",
       " (2,\n",
       "  '0.009*\"thing\" + 0.008*\"years\" + 0.008*\"hes\" + 0.006*\"way\" + 0.006*\"things\" + 0.006*\"gon\" + 0.006*\"life\" + 0.005*\"day\" + 0.005*\"thank\" + 0.005*\"theyre\"'),\n",
       " (3,\n",
       "  '0.008*\"life\" + 0.008*\"hes\" + 0.007*\"kids\" + 0.007*\"shes\" + 0.007*\"day\" + 0.006*\"thing\" + 0.006*\"cause\" + 0.005*\"george\" + 0.005*\"game\" + 0.005*\"way\"')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=50)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #3 (Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>david</th>\n",
       "      <td>comedy special david nihill complexities ident...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>state name martin moreno martinnnnn gabriel ig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>george</th>\n",
       "      <td>george carlin im glad im dead controversial pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jon</th>\n",
       "      <td>interview jon stewart george carlin talks vari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kate</th>\n",
       "      <td>whoa okay good okay dont embarrass expectation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kevin</th>\n",
       "      <td>kevin james kevin james standup comedy special...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leanne</th>\n",
       "      <td>leanne morgan woman im woman morgan delivers s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lewis</th>\n",
       "      <td>lewis black standup comedy show lewis black si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>louis ck dolby louis cks selfreleased standup ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matt</th>\n",
       "      <td>second hourlong comedy special matthew stephen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pete</th>\n",
       "      <td>pete davidson turbo fonzarelli date january ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roseanne</th>\n",
       "      <td>roseanne barr cancel standup comedy special ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sammy</th>\n",
       "      <td>humorous standup segment comedian sammy obeid ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shane</th>\n",
       "      <td>shane gillis standup comedy special live creek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stavros</th>\n",
       "      <td>standup comedy special live lodge room stavros...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 transcript\n",
       "david     comedy special david nihill complexities ident...\n",
       "gabriel   state name martin moreno martinnnnn gabriel ig...\n",
       "george    george carlin im glad im dead controversial pr...\n",
       "jon       interview jon stewart george carlin talks vari...\n",
       "kate      whoa okay good okay dont embarrass expectation...\n",
       "kevin     kevin james kevin james standup comedy special...\n",
       "leanne    leanne morgan woman im woman morgan delivers s...\n",
       "lewis     lewis black standup comedy show lewis black si...\n",
       "louis     louis ck dolby louis cks selfreleased standup ...\n",
       "matt      second hourlong comedy special matthew stephen...\n",
       "pete      pete davidson turbo fonzarelli date january ne...\n",
       "roseanne  roseanne barr cancel standup comedy special ro...\n",
       "sammy     humorous standup segment comedian sammy obeid ...\n",
       "shane     shane gillis standup comedy special live creek...\n",
       "stavros   standup comedy special live lodge room stavros..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaahhh</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abnormal</th>\n",
       "      <th>abolitionist</th>\n",
       "      <th>...</th>\n",
       "      <th>zionists</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zorro</th>\n",
       "      <th>zuck</th>\n",
       "      <th>zuckerbergs</th>\n",
       "      <th>álvarez</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>david</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>george</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jon</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kate</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kevin</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leanne</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lewis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pete</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roseanne</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sammy</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shane</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stavros</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 6062 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          aaaaah  aaaahhh  aaah  aah  abc  abilities  ability  able  abnormal  \\\n",
       "david          0        0     0    0    0          0        0     0         0   \n",
       "gabriel        0        0     1    0    1          0        0     0         0   \n",
       "george         0        0     0    0    0          0        1     3         0   \n",
       "jon            0        0     0    0    0          0        0     2         0   \n",
       "kate           0        0     0    1    0          0        0     2         0   \n",
       "kevin          0        0     0    0    0          0        0     0         0   \n",
       "leanne         0        0     0    0    0          0        0     0         0   \n",
       "lewis          0        0     0    0    0          0        0     1         4   \n",
       "louis          0        1     0    3    0          0        0     2         0   \n",
       "matt           0        0     0    0    0          0        0     3         0   \n",
       "pete           0        0     0    1    0          0        1     1         0   \n",
       "roseanne       0        0     0    0    4          1        0     8         0   \n",
       "sammy          0        0     0    0    0          0        0     0         0   \n",
       "shane          0        0     0    0    0          0        0     0         0   \n",
       "stavros        1        0     0    0    0          0        0     2         0   \n",
       "\n",
       "          abolitionist  ...  zionists  zip  zone  zones  zoo  zoom  zorro  \\\n",
       "david                1  ...         0    0     0      0    0     0      3   \n",
       "gabriel              0  ...         0    0     0      1    0     3      0   \n",
       "george               0  ...         0    0     0      0    0     0      0   \n",
       "jon                  0  ...         0    0     1      0    0     0      0   \n",
       "kate                 0  ...         0    0     0      0    0     0      0   \n",
       "kevin                0  ...         0    0     0      0    0     0      0   \n",
       "leanne               0  ...         0    1     0      0    0     0      0   \n",
       "lewis                0  ...         0    0     0      0    0     0      0   \n",
       "louis                0  ...         0    0     0      0    0     0      0   \n",
       "matt                 0  ...         0    0     1      0    0     0      0   \n",
       "pete                 0  ...         0    0     1      0    0     1      0   \n",
       "roseanne             0  ...         0    0     0      0    0     0      0   \n",
       "sammy                0  ...         1    0     0      0    0     0      0   \n",
       "shane                0  ...         0    0     0      0    1     0      0   \n",
       "stavros              0  ...         0    0     0      0    0     0      0   \n",
       "\n",
       "          zuck  zuckerbergs  álvarez  \n",
       "david        0            0        0  \n",
       "gabriel      0            0        3  \n",
       "george       1            1        0  \n",
       "jon          0            0        0  \n",
       "kate         0            0        0  \n",
       "kevin        0            0        0  \n",
       "leanne       0            0        0  \n",
       "lewis        0            0        0  \n",
       "louis        0            0        0  \n",
       "matt         0            0        0  \n",
       "pete         0            0        0  \n",
       "roseanne     0            0        0  \n",
       "sammy        0            0        0  \n",
       "shane        0            0        0  \n",
       "stavros      0            0        0  \n",
       "\n",
       "[15 rows x 6062 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names_out())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure: A corpus in Gensim is not a simple collection of text strings. It's typically represented as an iterable object, like a list or stream, where each element represents a single document.\n",
    "Documents: Each document within the corpus can be a string of text, a list of words, or even a more complex data structure containing additional information like metadata or annotations.\n",
    "Purpose: The corpus serves as the raw material for training various NLP models in Gensim. These models analyze the statistical relationships between words and documents within the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*\"jesus\" + 0.004*\"okay\" + 0.004*\"cause\" + 0.004*\"woman\" + 0.004*\"shit\" + 0.003*\"baby\" + 0.003*\"yall\" + 0.003*\"nice\" + 0.003*\"george\" + 0.003*\"fck\"'),\n",
       " (1,\n",
       "  '0.016*\"audience\" + 0.008*\"okay\" + 0.006*\"cause\" + 0.005*\"irish\" + 0.004*\"fuck\" + 0.004*\"mom\" + 0.004*\"shit\" + 0.004*\"ill\" + 0.003*\"um\" + 0.003*\"year\"')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=100)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"mom\" + 0.005*\"shit\" + 0.004*\"fck\" + 0.004*\"guys\" + 0.004*\"cause\" + 0.003*\"dude\" + 0.003*\"fcking\" + 0.003*\"okay\" + 0.003*\"ill\" + 0.003*\"nice\"'),\n",
       " (1,\n",
       "  '0.026*\"audience\" + 0.009*\"okay\" + 0.008*\"irish\" + 0.007*\"cause\" + 0.006*\"jesus\" + 0.005*\"um\" + 0.004*\"ill\" + 0.004*\"fuck\" + 0.004*\"red\" + 0.004*\"moment\"'),\n",
       " (2,\n",
       "  '0.006*\"okay\" + 0.005*\"cause\" + 0.005*\"yall\" + 0.005*\"baby\" + 0.003*\"husband\" + 0.003*\"year\" + 0.003*\"car\" + 0.003*\"money\" + 0.003*\"shit\" + 0.003*\"weeks\"')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 3 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=100)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.010*\"okay\" + 0.007*\"mom\" + 0.006*\"cause\" + 0.005*\"yall\" + 0.005*\"ill\" + 0.004*\"um\" + 0.004*\"ta\" + 0.004*\"guys\" + 0.004*\"husband\" + 0.004*\"baby\"'),\n",
       " (1,\n",
       "  '0.007*\"george\" + 0.006*\"shit\" + 0.005*\"carlin\" + 0.004*\"red\" + 0.004*\"fck\" + 0.004*\"okay\" + 0.004*\"asshole\" + 0.004*\"hard\" + 0.004*\"fcking\" + 0.004*\"fuck\"'),\n",
       " (2,\n",
       "  '0.019*\"audience\" + 0.007*\"cause\" + 0.005*\"irish\" + 0.005*\"okay\" + 0.004*\"jesus\" + 0.004*\"shit\" + 0.004*\"story\" + 0.004*\"country\" + 0.003*\"nice\" + 0.003*\"fuck\"'),\n",
       " (3,\n",
       "  '0.000*\"connections\" + 0.000*\"connected\" + 0.000*\"youtube\" + 0.000*\"hat\" + 0.000*\"recognize\" + 0.000*\"complex\" + 0.000*\"map\" + 0.000*\"math\" + 0.000*\"borders\" + 0.000*\"dinosaurs\"')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=100)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Topics in Each Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 9 topic models we looked at, the nouns and adjectives, 4 topic one made the most sense. So let's pull that down here and run it through some more iterations to get more fine-tuned topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.017*\"jesus\" + 0.008*\"cause\" + 0.006*\"fuck\" + 0.006*\"confidence\" + 0.005*\"nice\" + 0.005*\"okay\" + 0.004*\"story\" + 0.004*\"weird\" + 0.004*\"dead\" + 0.004*\"homeless\"'),\n",
       " (1,\n",
       "  '0.007*\"fuck\" + 0.006*\"okay\" + 0.006*\"mom\" + 0.006*\"shit\" + 0.005*\"fucking\" + 0.005*\"yall\" + 0.004*\"dude\" + 0.004*\"cause\" + 0.004*\"funny\" + 0.004*\"friend\"'),\n",
       " (2,\n",
       "  '0.011*\"okay\" + 0.005*\"cause\" + 0.005*\"ill\" + 0.004*\"um\" + 0.004*\"woman\" + 0.004*\"sex\" + 0.004*\"mom\" + 0.003*\"car\" + 0.003*\"shit\" + 0.003*\"sorry\"'),\n",
       " (3,\n",
       "  '0.026*\"audience\" + 0.008*\"irish\" + 0.006*\"cause\" + 0.005*\"george\" + 0.004*\"america\" + 0.004*\"american\" + 0.003*\"carlin\" + 0.003*\"ireland\" + 0.003*\"moment\" + 0.003*\"fck\"')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=100)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These four topics look pretty decent. Let's settle on these for now.\n",
    "* Topic 0: religion\n",
    "* Topic 1: friend\n",
    "* Topic 2: apology\n",
    "* Topic 3: politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 'david'),\n",
       " (2, 'gabriel'),\n",
       " (3, 'george'),\n",
       " (3, 'jon'),\n",
       " (2, 'kate'),\n",
       " (3, 'kevin'),\n",
       " (1, 'leanne'),\n",
       " (1, 'lewis'),\n",
       " (0, 'louis'),\n",
       " (1, 'matt'),\n",
       " (1, 'pete'),\n",
       " (2, 'roseanne'),\n",
       " (1, 'sammy'),\n",
       " (1, 'shane'),\n",
       " (2, 'stavros')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For a first pass of LDA, these kind of make sense to me, so we'll call it a day for now.\n",
    "* Topic 0: religion louis\n",
    "* Topic 1: friend lewis matt pete sammy shane\n",
    "* Topic 2: apology gabriel kate roseane stavros\n",
    "* Topic 3: politics david george jon kevin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Additional Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try further modifying the parameters of the topic models above and see if you can get better topics.\n",
    "2. Create a new topic model that includes terms from a different [part of speech](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and see if you can get better topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "Topic 0:\n",
      "mom fuck cause stalker okay weird terry fucking moms sorry\n",
      "Topic 1:\n",
      "cause mom okay room story uhh parents fucking sorry ill\n",
      "Topic 2:\n",
      "audience cause okay jesus shit irish woman ill sorry story\n",
      "Topic 3:\n",
      "okay shit cause fuck guys year ill white room fck\n",
      "Topic 4:\n",
      "um okay ill cameras kate cause tonight mirror whoa camera\n",
      "Topic 5:\n",
      "jesus cause audience fuck confidence story fucking okay auschwitz nice\n",
      "Topic 6:\n",
      "okay shit ill cause audience crazy news fck white money\n",
      "Topic 7:\n",
      "yall husband baby okay children lord money school bed woman\n",
      "Topic 8:\n",
      "audience irish cause ireland america moment laughing david american country\n",
      "Topic 9:\n",
      "audience okay cause shit ill mom guys room year funny\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Define the number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Further modify parameters to improve topics\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics,          # Number of topics\n",
    "                                      max_iter=10,                     # Maximum number of iterations\n",
    "                                      learning_method='online',        # Learning method\n",
    "                                      random_state=100,                # Random state for reproducibility\n",
    "                                      n_jobs=-1)                       # Use all available CPU cores\n",
    "\n",
    "# Fit the LDA model to the data\n",
    "lda_output = lda_model.fit_transform(data_cvna)\n",
    "\n",
    "# Display the top words for each topic\n",
    "def display_topics(model, feature_names, num_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
    "\n",
    "# Define the number of top words to display for each topic\n",
    "num_top_words = 10\n",
    "\n",
    "# Display the topics\n",
    "print(\"Topics found via LDA:\")\n",
    "display_topics(lda_model, cvna.get_feature_names_out(), num_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'example', 'sentence', 'containing', 'nouns', 'adjectives', 'verbs']\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def nouns_adjectives_verbs(text):\n",
    "    \"\"\"\n",
    "    Given a string of text, tokenize the text and pull out nouns, adjectives, and verbs.\n",
    "    \"\"\"\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Get the part-of-speech tags for each token\n",
    "    tagged_words = pos_tag(tokens)\n",
    "    # Define allowed POS tags for nouns, adjectives, and verbs\n",
    "    allowed_tags = ['NN', 'NNS', 'NNP', 'NNPS',    # Nouns\n",
    "                    'JJ', 'JJR', 'JJS',           # Adjectives\n",
    "                    'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']  # Verbs\n",
    "    # Filter tokens based on allowed POS tags\n",
    "    filtered_words = [word for word, tag in tagged_words if tag in allowed_tags]\n",
    "    return filtered_words\n",
    "\n",
    "# Example usage:\n",
    "text = \"This is an example sentence containing nouns, adjectives, and verbs.\"\n",
    "filtered_text = nouns_adjectives_verbs(text)\n",
    "print(filtered_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the code:\n",
    "\n",
    "This code defines a function `nouns_adjectives_verbs` that takes a string of text as input and returns a list of nouns, adjectives, and verbs found in the text.\n",
    "\n",
    "Here's a breakdown of the steps:\n",
    "\n",
    "1. **Import libraries:**\n",
    "    * `nltk.pos_tag`: Used for Part-of-Speech (POS) tagging.\n",
    "    * `nltk.word_tokenize`: Used for tokenizing the text into words.\n",
    "    * `nltk.corpus.stopwords` (not used in this specific function): Might be intended for future implementation to remove stop words (common words like \"the\", \"a\").\n",
    "\n",
    "2. **Function definition:**\n",
    "    * `nouns_adjectives_verbs(text)`: This function takes a string `text` as input.\n",
    "\n",
    "3. **Tokenization:**\n",
    "    * `tokens = word_tokenize(text)`: Splits the text into a list of individual words.\n",
    "\n",
    "4. **POS Tagging:**\n",
    "    * `tagged_words = pos_tag(tokens)`: Assigns POS tags (e.g., noun, verb, adjective) to each word in the `tokens` list. The output is a list of tuples where each tuple contains a word and its corresponding POS tag.\n",
    "\n",
    "5. **Filtering by POS tags:**\n",
    "    * `allowed_tags`: Defines a list of POS tags that represent nouns, adjectives, and verbs.\n",
    "    * `filtered_words = [word for word, tag in tagged_words if tag in allowed_tags]`: This list comprehension iterates through the `tagged_words` list. It includes only those words where the corresponding POS tag (`tag`) is present in the `allowed_tags` list. This effectively filters the words based on their grammatical categories.\n",
    "\n",
    "6. **Returning the filtered list:**\n",
    "    * The function returns the `filtered_words` list, which contains only nouns, adjectives, and verbs from the original text.\n",
    "\n",
    "7. **Example usage:**\n",
    "    * Demonstrates how to call the function with a sample sentence.\n",
    "    * Prints the filtered list containing only nouns, adjectives, and verbs from the example sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
